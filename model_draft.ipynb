{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be213e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmTokenizer, EsmModel\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e3676",
   "metadata": {},
   "source": [
    "# üß¨ Protein Aggregation Prediction Model\n",
    "\n",
    "## Overview\n",
    "This notebook implements a deep learning model that predicts protein aggregation behavior by combining:\n",
    "1. **Protein sequence information** using ESM2 transformer embeddings\n",
    "2. **Environmental conditions** (temperature, pH, concentration) through neural network processing  \n",
    "3. **Multimodal fusion** to create unified representations for downstream prediction\n",
    "\n",
    "## Architecture Pipeline\n",
    "```\n",
    "Protein Sequence ‚Üí ESM2 ‚Üí Attention Pooling ‚Üí [1280 dims]\n",
    "                                                    ‚Üì\n",
    "Environmental Data ‚Üí MLP + BatchNorm ‚Üí [16 dims]   ‚Üì\n",
    "                                                    ‚Üì\n",
    "                          Fusion (Concatenation) ‚Üí [1296 dims] ‚Üí Prediction Head\n",
    "```\n",
    "\n",
    "## Sections\n",
    "1. **Sequence Embedding**: Process protein sequences with ESM2 and attention pooling\n",
    "2. **Environmental Embedding**: Neural network processing of environmental conditions\n",
    "3. **Fusion**: Combine protein and environmental representations\n",
    "4. **[Future]**: Classification/regression heads for aggregation prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660dc15",
   "metadata": {},
   "source": [
    "# Sequence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff1a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_sequence = [\"DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59619102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmModel(\n",
      "  (embeddings): EsmEmbeddings(\n",
      "    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): EsmEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-32): 33 x EsmLayer(\n",
      "        (attention): EsmAttention(\n",
      "          (self): EsmSelfAttention(\n",
      "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (rotary_embeddings): RotaryEmbedding()\n",
      "          )\n",
      "          (output): EsmSelfOutput(\n",
      "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (intermediate): EsmIntermediate(\n",
      "          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        )\n",
      "        (output): EsmOutput(\n",
      "          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): EsmPooler(\n",
      "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (contact_head): EsmContactPredictionHead(\n",
      "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "    (activation): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = EsmModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "print(model)  # Prints full architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce1062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0177f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(protein_sequence, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a484219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([1, 42, 1280])\n",
      "Pooler output shape: torch.Size([1, 1280])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "print(f\"Model output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"Pooler output shape: {outputs.pooler_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50d690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 42, 1280])\n",
      "Pooled shape: torch.Size([1, 1280])\n",
      "Attention weights shape: torch.Size([1, 42])\n",
      "Attention weights sum (should be ~1.0): tensor([1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "Attention weights for each position:\n",
      "Position 0: 0.0224\n",
      "Position 1: 0.0219\n",
      "Position 2: 0.0253\n",
      "Position 3: 0.0255\n",
      "Position 4: 0.0247\n",
      "Position 5: 0.0228\n",
      "Position 6: 0.0231\n",
      "Position 7: 0.0231\n",
      "Position 8: 0.0247\n",
      "Position 9: 0.0248\n",
      "Position 10: 0.0239\n",
      "Position 11: 0.0252\n",
      "Position 12: 0.0230\n",
      "Position 13: 0.0224\n",
      "Position 14: 0.0236\n",
      "Position 15: 0.0267\n",
      "Position 16: 0.0262\n",
      "Position 17: 0.0255\n",
      "Position 18: 0.0222\n",
      "Position 19: 0.0260\n",
      "Position 20: 0.0251\n",
      "Position 21: 0.0242\n",
      "Position 22: 0.0255\n",
      "Position 23: 0.0233\n",
      "Position 24: 0.0217\n",
      "Position 25: 0.0230\n",
      "Position 26: 0.0230\n",
      "Position 27: 0.0239\n",
      "Position 28: 0.0246\n",
      "Position 29: 0.0249\n",
      "Position 30: 0.0244\n",
      "Position 31: 0.0229\n",
      "Position 32: 0.0208\n",
      "Position 33: 0.0238\n",
      "Position 34: 0.0254\n",
      "Position 35: 0.0247\n",
      "Position 36: 0.0210\n",
      "Position 37: 0.0232\n",
      "Position 38: 0.0257\n",
      "Position 39: 0.0228\n",
      "Position 40: 0.0219\n",
      "Position 41: 0.0213\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention pooling layer to reduce sequence embeddings to a fixed size.\n",
    "    Reduces [batch_size, sequence_length, hidden_dim] -> [batch_size, hidden_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, embeddings, attention_mask=None):\n",
    "        # embeddings: [batch_size, sequence_length, hidden_dim]\n",
    "        # attention_mask: [batch_size, sequence_length] - 1 for valid tokens, 0 for padding\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention(embeddings)  # [batch_size, sequence_length, 1]\n",
    "        attention_scores = attention_scores.squeeze(-1)  # [batch_size, sequence_length]\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, sequence_length]\n",
    "        \n",
    "        # Apply attention weights to embeddings\n",
    "        pooled_output = torch.sum(embeddings * attention_weights.unsqueeze(-1), dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return pooled_output, attention_weights\n",
    "\n",
    "# Get the hidden states from ESM2 model output\n",
    "hidden_states = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_dim]\n",
    "print(f\"Original shape: {hidden_states.shape}\")\n",
    "\n",
    "# Initialize attention pooling layer\n",
    "hidden_dim = hidden_states.shape[-1]  # Get hidden dimension from ESM2 output\n",
    "attention_pooler = AttentionPooling(hidden_dim)\n",
    "\n",
    "# Apply attention pooling\n",
    "# Get attention mask from tokenizer output (to handle padding)\n",
    "attention_mask = tokens.get('attention_mask', None)\n",
    "pooled_embedding, attention_weights = attention_pooler(hidden_states, attention_mask)\n",
    "\n",
    "print(f\"Pooled shape: {pooled_embedding.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Attention weights sum (should be ~1.0): {attention_weights.sum(dim=1)}\")\n",
    "\n",
    "# Visualize which positions get the most attention\n",
    "print(\"\\nAttention weights for each position:\")\n",
    "for i, weight in enumerate(attention_weights[0]):  # Show weights for first sequence in batch\n",
    "    print(f\"Position {i}: {weight.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b7c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Statistics:\n",
      "mean: 0.0238\n",
      "std: 0.0015\n",
      "min: 0.0208\n",
      "max: 0.0267\n",
      "entropy: 3.7357\n",
      "\n",
      "Sequence embedding shape: (1, 1280)\n",
      "Sequence embedding sample values: [-6.2805954e-03  4.5681186e-02 -1.4986906e-02  3.5999343e-05\n",
      " -1.3745151e-02 -1.2572345e-03 -7.8304186e-02  8.1339180e-02\n",
      "  1.5063080e-01 -5.1894628e-02]\n"
     ]
    }
   ],
   "source": [
    "# Additional utility functions for attention pooling\n",
    "\n",
    "def visualize_attention_weights(attention_weights, sequence, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weights across the protein sequence\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Convert sequence list to string if needed\n",
    "    if isinstance(sequence, list):\n",
    "        sequence = sequence[0]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    positions = range(len(attention_weights[0]))\n",
    "    weights = attention_weights[0].detach().numpy()\n",
    "    \n",
    "    bars = ax.bar(positions, weights)\n",
    "    ax.set_xlabel('Sequence Position')\n",
    "    ax.set_ylabel('Attention Weight')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add amino acid labels if sequence is short enough\n",
    "    if len(sequence) <= 50:\n",
    "        ax.set_xticks(positions[1:-1])  # Skip special tokens (CLS and SEP)\n",
    "        ax.set_xticklabels([sequence[i-1] for i in positions[1:-1]], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def get_attention_statistics(attention_weights):\n",
    "    \"\"\"\n",
    "    Get statistics about attention distribution\n",
    "    \"\"\"\n",
    "    weights = attention_weights[0].detach().numpy()\n",
    "    stats = {\n",
    "        'mean': np.mean(weights),\n",
    "        'std': np.std(weights),\n",
    "        'min': np.min(weights),\n",
    "        'max': np.max(weights),\n",
    "        'entropy': -np.sum(weights * np.log(weights + 1e-8))  # Shannon entropy\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Apply utilities to our current results\n",
    "print(\"Attention Statistics:\")\n",
    "stats = get_attention_statistics(attention_weights)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Store the pooled embedding for later use\n",
    "sequence_embedding = pooled_embedding.detach().numpy()\n",
    "print(f\"\\nSequence embedding shape: {sequence_embedding.shape}\")\n",
    "print(f\"Sequence embedding sample values: {sequence_embedding[0][:10]}\")  # Show first 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9b017",
   "metadata": {},
   "source": [
    "## üìù Sequence Embedding Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "- **ESM2 Model Loading**: Successfully loaded the `facebook/esm2_t33_650M_UR50D` pre-trained protein language model\n",
    "- **Tokenization**: Converted protein sequence to tokens compatible with ESM2\n",
    "- **Feature Extraction**: Generated contextualized embeddings `[1, 42, 1280]` for each amino acid position\n",
    "- **Attention Pooling**: Implemented learnable attention mechanism to reduce sequence dimension from `[1, 42, 1280]` ‚Üí `[1, 1280]`\n",
    "- **Interpretability**: Added utilities to analyze attention weights and understand which positions are most important\n",
    "\n",
    "### Key Benefits:\n",
    "- ‚úÖ **Contextual Understanding**: ESM2 captures protein structure and function relationships\n",
    "- ‚úÖ **Adaptive Pooling**: Attention learns task-specific sequence importance\n",
    "- ‚úÖ **Fixed Output Size**: Consistent 1280-dimensional representation regardless of sequence length\n",
    "- ‚úÖ **Gradient Flow**: End-to-end trainable for protein aggregation prediction\n",
    "\n",
    "**Output**: Ready-to-use 1280-dimensional protein sequence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea13aff",
   "metadata": {},
   "source": [
    "# Environmental Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85bffb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature (¬∞C)   pH  Protein concentration (¬µM)\n",
      "0              37.0  7.4                        50.0\n"
     ]
    }
   ],
   "source": [
    "environmental_conditions = {\n",
    "    \"Temperature (¬∞C)\": 37.0,\n",
    "    \"pH\": 7.4,\n",
    "    \"Protein concentration (¬µM)\": 50.0,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([environmental_conditions])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded3218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environmental features shape: torch.Size([1, 3])\n",
      "Environmental features: tensor([[37.0000,  7.4000, 50.0000]])\n",
      "Environmental embedding shape: torch.Size([1, 16])\n",
      "Environmental embedding sample values: tensor([14.5434,  0.0000,  0.0000,  4.5256, 20.9454,  3.7892,  4.7910,  0.0000,\n",
      "         0.0000,  5.6818], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Environmental embedding layer parameters:\n",
      "Linear layer weight shape: torch.Size([16, 3])\n",
      "Linear layer bias shape: torch.Size([16])\n",
      "Batch norm running mean shape: torch.Size([16])\n",
      "Batch norm running var shape: torch.Size([16])\n",
      "\n",
      "Note: For training, use env_embedding_layer.train() and batch_size > 1\n"
     ]
    }
   ],
   "source": [
    "class EnvironmentalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Environmental embedding layer with batch normalization.\n",
    "    Converts environmental features to a 16-dimensional embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=3, output_dim=16):\n",
    "        super(EnvironmentalEmbedding, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, env_features):\n",
    "        # env_features: [batch_size, input_dim]\n",
    "        x = self.linear(env_features)  # [batch_size, output_dim]\n",
    "        x = self.batch_norm(x)         # Apply batch normalization\n",
    "        x = self.activation(x)         # Apply ReLU activation\n",
    "        return x\n",
    "\n",
    "# Convert environmental conditions to tensor\n",
    "env_features = torch.tensor(df.values, dtype=torch.float32)\n",
    "print(f\"Environmental features shape: {env_features.shape}\")\n",
    "print(f\"Environmental features: {env_features}\")\n",
    "\n",
    "# Initialize environmental embedding layer\n",
    "env_embedding_layer = EnvironmentalEmbedding(input_dim=3, output_dim=16)\n",
    "\n",
    "# Apply environmental embedding\n",
    "# Set to evaluation mode for single sample inference (batch norm issue)\n",
    "env_embedding_layer.eval()\n",
    "env_embedding = env_embedding_layer(env_features)\n",
    "print(f\"Environmental embedding shape: {env_embedding.shape}\")\n",
    "print(f\"Environmental embedding sample values: {env_embedding[0][:10]}\")\n",
    "\n",
    "# Display layer parameters\n",
    "print(f\"\\nEnvironmental embedding layer parameters:\")\n",
    "print(f\"Linear layer weight shape: {env_embedding_layer.linear.weight.shape}\")\n",
    "print(f\"Linear layer bias shape: {env_embedding_layer.linear.bias.shape}\")\n",
    "print(f\"Batch norm running mean shape: {env_embedding_layer.batch_norm.running_mean.shape}\")\n",
    "print(f\"Batch norm running var shape: {env_embedding_layer.batch_norm.running_var.shape}\")\n",
    "\n",
    "# For training with larger batches, you would use:\n",
    "print(f\"\\nNote: For training, use env_embedding_layer.train() and batch_size > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae4907bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch environmental conditions:\n",
      "   Temperature (¬∞C)   pH  Protein concentration (¬µM)\n",
      "0              37.0  7.4                        50.0\n",
      "1              25.0  6.8                       100.0\n",
      "2              42.0  8.0                        25.0\n",
      "3              30.0  7.0                        75.0\n",
      "\n",
      "Batch tensor shape: torch.Size([4, 3])\n",
      "\n",
      "Batch environmental embeddings shape: torch.Size([4, 16])\n",
      "Sample embeddings for first condition: tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Sample embeddings for second condition: tensor([1.3583, 1.3393, 1.3365, 1.3597, 1.3444], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Single sample embedding stored for fusion: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: Environmental embedding with multiple samples (batch processing)\n",
    "\n",
    "# Create sample batch of environmental conditions\n",
    "batch_env_conditions = [\n",
    "    {\"Temperature (¬∞C)\": 37.0, \"pH\": 7.4, \"Protein concentration (¬µM)\": 50.0},\n",
    "    {\"Temperature (¬∞C)\": 25.0, \"pH\": 6.8, \"Protein concentration (¬µM)\": 100.0},\n",
    "    {\"Temperature (¬∞C)\": 42.0, \"pH\": 8.0, \"Protein concentration (¬µM)\": 25.0},\n",
    "    {\"Temperature (¬∞C)\": 30.0, \"pH\": 7.0, \"Protein concentration (¬µM)\": 75.0}\n",
    "]\n",
    "\n",
    "batch_df = pd.DataFrame(batch_env_conditions)\n",
    "batch_env_features = torch.tensor(batch_df.values, dtype=torch.float32)\n",
    "\n",
    "print(\"Batch environmental conditions:\")\n",
    "print(batch_df)\n",
    "print(f\"\\nBatch tensor shape: {batch_env_features.shape}\")\n",
    "\n",
    "# Create new embedding layer for training demonstration\n",
    "env_embedding_layer_batch = EnvironmentalEmbedding(input_dim=3, output_dim=16)\n",
    "env_embedding_layer_batch.train()  # Set to training mode\n",
    "\n",
    "# Process batch\n",
    "batch_env_embedding = env_embedding_layer_batch(batch_env_features)\n",
    "print(f\"\\nBatch environmental embeddings shape: {batch_env_embedding.shape}\")\n",
    "print(f\"Sample embeddings for first condition: {batch_env_embedding[0][:5]}\")\n",
    "print(f\"Sample embeddings for second condition: {batch_env_embedding[1][:5]}\")\n",
    "\n",
    "# Store the single sample embedding for later fusion\n",
    "env_embedding_single = env_embedding.detach()\n",
    "print(f\"\\nSingle sample embedding stored for fusion: {env_embedding_single.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb78440",
   "metadata": {},
   "source": [
    "## üìù Environmental Embedding Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "- **Feature Engineering**: Processed 3 key environmental factors (Temperature, pH, Protein concentration)\n",
    "- **Neural Network Layer**: Implemented MLP with Linear ‚Üí BatchNorm ‚Üí ReLU architecture\n",
    "- **Dimensionality Expansion**: Transformed 3 environmental features ‚Üí 16-dimensional embedding\n",
    "- **Batch Processing**: Handled both single samples (.eval() mode) and batch training (.train() mode)\n",
    "- **Stability Features**: Added batch normalization for consistent training dynamics\n",
    "\n",
    "### Key Benefits:\n",
    "- ‚úÖ **Feature Learning**: Neural network learns optimal environmental representations\n",
    "- ‚úÖ **Batch Normalization**: Improved training stability and convergence speed  \n",
    "- ‚úÖ **Non-linear Processing**: ReLU activation enables complex feature interactions\n",
    "- ‚úÖ **Scalable Design**: Easy to add more environmental factors as inputs\n",
    "- ‚úÖ **Training Ready**: Proper handling of single-sample inference and batch training\n",
    "\n",
    "**Output**: Ready-to-use 16-dimensional environmental embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e07f1",
   "metadata": {},
   "source": [
    "# Protein + Environment Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c517b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embedding Fusion ===\n",
      "Sequence embedding shape: torch.Size([1, 1280])\n",
      "Environmental embedding shape: torch.Size([1, 16])\n",
      "Fused embedding shape: torch.Size([1, 1296])\n",
      "Expected fused dimension: 1296\n",
      "\n",
      "=== Fusion Components ===\n",
      "Sequence portion (first 5 values): tensor([-6.2806e-03,  4.5681e-02, -1.4987e-02,  3.5999e-05, -1.3745e-02])\n",
      "Environmental portion (last 5 values): tensor([ 4.4430,  0.0000, 19.4043,  0.0000, 11.8405])\n",
      "\n",
      "=== Verification ===\n",
      "Original sequence sample: tensor([-6.2806e-03,  4.5681e-02, -1.4987e-02,  3.5999e-05, -1.3745e-02])\n",
      "Original env sample: tensor([ 4.4430,  0.0000, 19.4043,  0.0000, 11.8405])\n",
      "Fused sequence portion matches: True\n",
      "Fused env portion matches: True\n",
      "\n",
      "Fused embedding stored for downstream tasks: torch.Size([1, 1296])\n"
     ]
    }
   ],
   "source": [
    "class ProteinEnvironmentFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fusion module for combining protein sequence embeddings with environmental embeddings.\n",
    "    Uses concatenation to combine the features.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_dim=1280, env_dim=16):\n",
    "        super(ProteinEnvironmentFusion, self).__init__()\n",
    "        self.sequence_dim = sequence_dim\n",
    "        self.env_dim = env_dim\n",
    "        self.fused_dim = sequence_dim + env_dim\n",
    "        \n",
    "    def forward(self, sequence_embedding, env_embedding):\n",
    "        # sequence_embedding: [batch_size, sequence_dim]\n",
    "        # env_embedding: [batch_size, env_dim]\n",
    "        \n",
    "        # Ensure both embeddings have the same batch size\n",
    "        assert sequence_embedding.shape[0] == env_embedding.shape[0], \\\n",
    "            f\"Batch size mismatch: sequence {sequence_embedding.shape[0]} vs env {env_embedding.shape[0]}\"\n",
    "        \n",
    "        # Concatenate along feature dimension\n",
    "        fused_embedding = torch.cat([sequence_embedding, env_embedding], dim=1)\n",
    "        # Output: [batch_size, sequence_dim + env_dim]\n",
    "        \n",
    "        return fused_embedding\n",
    "\n",
    "# Initialize fusion module\n",
    "fusion_module = ProteinEnvironmentFusion(sequence_dim=1280, env_dim=16)\n",
    "\n",
    "# Get the current embeddings (convert sequence embedding back to tensor)\n",
    "sequence_embedding_tensor = torch.tensor(sequence_embedding, dtype=torch.float32)\n",
    "env_embedding_tensor = env_embedding_single\n",
    "\n",
    "print(\"=== Embedding Fusion ===\")\n",
    "print(f\"Sequence embedding shape: {sequence_embedding_tensor.shape}\")\n",
    "print(f\"Environmental embedding shape: {env_embedding_tensor.shape}\")\n",
    "\n",
    "# Apply fusion\n",
    "fused_embedding = fusion_module(sequence_embedding_tensor, env_embedding_tensor)\n",
    "print(f\"Fused embedding shape: {fused_embedding.shape}\")\n",
    "print(f\"Expected fused dimension: {fusion_module.fused_dim}\")\n",
    "\n",
    "# Show sample values from each component\n",
    "print(\"\\n=== Fusion Components ===\")\n",
    "print(f\"Sequence portion (first 5 values): {fused_embedding[0][:5]}\")\n",
    "print(f\"Environmental portion (last 5 values): {fused_embedding[0][-5:]}\")\n",
    "\n",
    "# Verify the concatenation is correct\n",
    "print(\"\\n=== Verification ===\")\n",
    "print(f\"Original sequence sample: {sequence_embedding_tensor[0][:5]}\")\n",
    "print(f\"Original env sample: {env_embedding_tensor[0][-5:]}\")\n",
    "print(f\"Fused sequence portion matches: {torch.allclose(fused_embedding[0][:1280], sequence_embedding_tensor[0])}\")\n",
    "print(f\"Fused env portion matches: {torch.allclose(fused_embedding[0][1280:], env_embedding_tensor[0])}\")\n",
    "\n",
    "# Store for downstream use\n",
    "protein_env_fused = fused_embedding.detach()\n",
    "print(f\"\\nFused embedding stored for downstream tasks: {protein_env_fused.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b315c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Fusion Demonstration ===\n",
      "Mock sequence batch shape: torch.Size([4, 1280])\n",
      "Environmental batch shape: torch.Size([4, 16])\n",
      "Batch fused embedding shape: torch.Size([4, 1296])\n",
      "\n",
      "=== Per-Sample Breakdown ===\n",
      "Sample 1 - Seq: [ 1.3823773  -0.02419936 -1.7813597  -1.0548543   0.82001823]\n",
      "Sample 1 - Env: [0.         0.42615467 0.43299773 0.         0.51579916]\n",
      "\n",
      "Sample 2 - Seq: [-1.0216062   0.11305421 -2.934657   -0.71725214  1.216467  ]\n",
      "Sample 2 - Env: [1.3577632 0.        0.        1.3295695 0.       ]\n",
      "\n",
      "Sample 3 - Seq: [-0.26486784  0.08872905  0.88852507  0.43666533  1.8734921 ]\n",
      "Sample 3 - Env: [0.        1.3483403 1.3459644 0.        1.3108011]\n",
      "\n",
      "Sample 4 - Seq: [0.24590443 0.86854786 0.01486731 1.3056583  2.0413456 ]\n",
      "Sample 4 - Env: [0.41502786 0.         0.         0.47161758 0.        ]\n",
      "\n",
      "=== Fusion Summary ===\n",
      "Input dimensions: Sequence (1280) + Environment (16)\n",
      "Output dimension: 1296\n",
      "Feature concatenation: [seq_features | env_features]\n",
      "Ready for downstream prediction layers!\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: Batch fusion processing\n",
    "\n",
    "# Create mock batch of sequence embeddings (normally these would come from attention pooling)\n",
    "batch_size = 4\n",
    "sequence_dim = 1280\n",
    "mock_sequence_batch = torch.randn(batch_size, sequence_dim)\n",
    "\n",
    "# Use the batch environmental embeddings we created earlier\n",
    "# batch_env_embedding was created in the previous environmental embedding cell\n",
    "\n",
    "print(\"=== Batch Fusion Demonstration ===\")\n",
    "print(f\"Mock sequence batch shape: {mock_sequence_batch.shape}\")\n",
    "print(f\"Environmental batch shape: {batch_env_embedding.shape}\")\n",
    "\n",
    "# Apply batch fusion\n",
    "batch_fused_embedding = fusion_module(mock_sequence_batch, batch_env_embedding)\n",
    "print(f\"Batch fused embedding shape: {batch_fused_embedding.shape}\")\n",
    "\n",
    "# Show how each sample in the batch gets processed\n",
    "print(\"\\n=== Per-Sample Breakdown ===\")\n",
    "for i in range(batch_size):\n",
    "    seq_portion = batch_fused_embedding[i][:5]  # First 5 values (from sequence)\n",
    "    env_portion = batch_fused_embedding[i][-5:]  # Last 5 values (from environment)\n",
    "    print(f\"Sample {i+1} - Seq: {seq_portion.detach().numpy()}\")\n",
    "    print(f\"Sample {i+1} - Env: {env_portion.detach().numpy()}\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=== Fusion Summary ===\")\n",
    "print(f\"Input dimensions: Sequence ({sequence_dim}) + Environment ({fusion_module.env_dim})\")\n",
    "print(f\"Output dimension: {fusion_module.fused_dim}\")\n",
    "print(f\"Feature concatenation: [seq_features | env_features]\")\n",
    "print(f\"Ready for downstream prediction layers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40cbbe",
   "metadata": {},
   "source": [
    "## üìù Protein + Environment Fusion Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "- **Multimodal Integration**: Successfully combined protein sequence and environmental embeddings\n",
    "- **Concatenation Fusion**: Implemented simple but effective feature concatenation strategy\n",
    "- **Dimension Mapping**: `[1280 sequence] + [16 environmental] = [1296 fused]` representation\n",
    "- **Batch Processing**: Demonstrated fusion works with both single samples and batches\n",
    "- **Verification System**: Built-in checks ensure proper concatenation and data integrity\n",
    "\n",
    "### Architecture Flow:\n",
    "1. **Protein Sequence**: ESM2 ‚Üí Attention Pooling ‚Üí `[batch_size, 1280]`\n",
    "2. **Environmental**: Raw features ‚Üí MLP + BatchNorm ‚Üí `[batch_size, 16]`  \n",
    "3. **Fusion**: Concatenation ‚Üí `[batch_size, 1296]`\n",
    "\n",
    "### Key Benefits:\n",
    "- ‚úÖ **Information Preservation**: Both modalities retain full representational power\n",
    "- ‚úÖ **Learnable Integration**: Downstream layers can learn optimal feature combinations\n",
    "- ‚úÖ **Batch Compatibility**: Seamless processing for training and inference\n",
    "- ‚úÖ **Gradient Flow**: End-to-end differentiable for joint optimization\n",
    "- ‚úÖ **Extensible Design**: Easy to add more input modalities\n",
    "\n",
    "**Output**: Complete 1296-dimensional multimodal embedding ready for aggregation prediction\n",
    "\n",
    "### Next Steps:\n",
    "- Add classification/regression heads for aggregation prediction\n",
    "- Implement training loops with aggregation datasets  \n",
    "- Add regularization and optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd1046",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b384eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aggregation Predictor Architecture ===\n",
      "AggregationPredictor(\n",
      "  (block1): Sequential(\n",
      "    (0): Linear(in_features=1296, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 373,314\n",
      "Trainable parameters: 373,314\n",
      "\n",
      "=== Prediction Test ===\n",
      "Input shape: torch.Size([1, 1296])\n",
      "Output logits shape: torch.Size([1, 2])\n",
      "Output logits: tensor([[-0.0103,  0.0952]])\n",
      "Prediction probabilities: tensor([[0.4737, 0.5263]])\n",
      "Predicted class: Aggregates\n",
      "Confidence: 0.5263\n",
      "\n",
      "=== Layer-by-layer Processing ===\n",
      "Input: torch.Size([1, 1296])\n",
      "After Block 1: torch.Size([1, 256])\n",
      "After Block 2: torch.Size([1, 128])\n",
      "After Block 3: torch.Size([1, 64])\n",
      "After Block 4: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "class AggregationPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for protein aggregation prediction.\n",
    "    Takes fused embeddings and predicts aggregation outcomes through 4 blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1296, dropout_rate=0.3):\n",
    "        super(AggregationPredictor, self).__init__()\n",
    "        \n",
    "        # Block 1: 1296 -> 256\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Block 2: 256 -> 128\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Block 3: 128 -> 64\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Block 4: 64 -> 2 (binary classification: aggregates/doesn't aggregate)\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(64, 2),\n",
    "            # Note: No activation here - will apply softmax in training/inference\n",
    "        )\n",
    "        \n",
    "    def forward(self, fused_embedding):\n",
    "        # fused_embedding: [batch_size, 1296]\n",
    "        \n",
    "        x = self.block1(fused_embedding)  # [batch_size, 256]\n",
    "        x = self.block2(x)               # [batch_size, 128]\n",
    "        x = self.block3(x)               # [batch_size, 64]\n",
    "        logits = self.block4(x)          # [batch_size, 2]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize the prediction model\n",
    "predictor = AggregationPredictor(input_dim=1296, dropout_rate=0.3)\n",
    "\n",
    "# Display model architecture\n",
    "print(\"=== Aggregation Predictor Architecture ===\")\n",
    "print(predictor)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in predictor.parameters())\n",
    "trainable_params = sum(p.numel() for p in predictor.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test with our fused embedding\n",
    "print(f\"\\n=== Prediction Test ===\")\n",
    "print(f\"Input shape: {protein_env_fused.shape}\")\n",
    "\n",
    "# Set model to evaluation mode for inference\n",
    "predictor.eval()\n",
    "with torch.no_grad():\n",
    "    prediction_logits = predictor(protein_env_fused)\n",
    "    \n",
    "print(f\"Output logits shape: {prediction_logits.shape}\")\n",
    "print(f\"Output logits: {prediction_logits}\")\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "prediction_probs = F.softmax(prediction_logits, dim=1)\n",
    "print(f\"Prediction probabilities: {prediction_probs}\")\n",
    "print(f\"Predicted class: {'Aggregates' if prediction_probs[0][1] > 0.5 else 'No Aggregation'}\")\n",
    "print(f\"Confidence: {prediction_probs[0].max().item():.4f}\")\n",
    "\n",
    "# Show layer-by-layer output shapes\n",
    "print(f\"\\n=== Layer-by-layer Processing ===\")\n",
    "predictor.train()  # Set to training mode to see intermediate outputs\n",
    "x = protein_env_fused\n",
    "print(f\"Input: {x.shape}\")\n",
    "\n",
    "x = predictor.block1(x)\n",
    "print(f\"After Block 1: {x.shape}\")\n",
    "\n",
    "x = predictor.block2(x)\n",
    "print(f\"After Block 2: {x.shape}\")\n",
    "\n",
    "x = predictor.block3(x)\n",
    "print(f\"After Block 3: {x.shape}\")\n",
    "\n",
    "x = predictor.block4(x)\n",
    "print(f\"After Block 4: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df53f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Processing Demonstration ===\n",
      "Batch fused embedding shape: torch.Size([4, 1296])\n",
      "Batch predictions shape: torch.Size([4, 2])\n",
      "Batch probabilities shape: torch.Size([4, 2])\n",
      "\n",
      "=== Individual Sample Predictions ===\n",
      "Sample 1:\n",
      "  Logits: [0.0808, 0.0808]\n",
      "  Probabilities: [No Agg: 0.5000, Agg: 0.5000]\n",
      "  Prediction: No Aggregation (confidence: 0.5000)\n",
      "\n",
      "Sample 2:\n",
      "  Logits: [0.0005, 0.1293]\n",
      "  Probabilities: [No Agg: 0.4678, Agg: 0.5322]\n",
      "  Prediction: Aggregates (confidence: 0.5322)\n",
      "\n",
      "Sample 3:\n",
      "  Logits: [0.0067, 0.1007]\n",
      "  Probabilities: [No Agg: 0.4765, Agg: 0.5235]\n",
      "  Prediction: Aggregates (confidence: 0.5235)\n",
      "\n",
      "Sample 4:\n",
      "  Logits: [0.0105, 0.1183]\n",
      "  Probabilities: [No Agg: 0.4731, Agg: 0.5269]\n",
      "  Prediction: Aggregates (confidence: 0.5269)\n",
      "\n",
      "=== Prediction Analysis Results ===\n",
      "Sample 1: No Aggregation (0.500 confidence)\n",
      "Sample 2: Aggregates (0.532 confidence)\n",
      "Sample 3: Aggregates (0.523 confidence)\n",
      "Sample 4: Aggregates (0.527 confidence)\n",
      "\n",
      "=== Model Summary ===\n",
      "Predictor ready for training with 373,314 trainable parameters\n",
      "Input: Fused embeddings [batch_size, 1296]\n",
      "Output: Aggregation logits [batch_size, 2]\n",
      "Architecture: 1296 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 2\n"
     ]
    }
   ],
   "source": [
    "# Batch processing demonstration\n",
    "print(\"=== Batch Processing Demonstration ===\")\n",
    "\n",
    "# Use the batch fused embeddings from earlier\n",
    "print(f\"Batch fused embedding shape: {batch_fused_embedding.shape}\")\n",
    "\n",
    "# Apply predictor to batch\n",
    "predictor.eval()\n",
    "with torch.no_grad():\n",
    "    batch_logits = predictor(batch_fused_embedding)\n",
    "    batch_probs = F.softmax(batch_logits, dim=1)\n",
    "\n",
    "print(f\"Batch predictions shape: {batch_logits.shape}\")\n",
    "print(f\"Batch probabilities shape: {batch_probs.shape}\")\n",
    "\n",
    "# Show predictions for each sample in batch\n",
    "print(f\"\\n=== Individual Sample Predictions ===\")\n",
    "for i in range(batch_fused_embedding.shape[0]):\n",
    "    logits = batch_logits[i]\n",
    "    probs = batch_probs[i]\n",
    "    predicted_class = \"Aggregates\" if probs[1] > 0.5 else \"No Aggregation\"\n",
    "    confidence = probs.max().item()\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Logits: [{logits[0]:.4f}, {logits[1]:.4f}]\")\n",
    "    print(f\"  Probabilities: [No Agg: {probs[0]:.4f}, Agg: {probs[1]:.4f}]\")\n",
    "    print(f\"  Prediction: {predicted_class} (confidence: {confidence:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Utility functions for prediction analysis\n",
    "def analyze_prediction(logits):\n",
    "    \"\"\"\n",
    "    Analyze prediction logits and return interpretable results\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1)\n",
    "    confidence = torch.max(probs, dim=1)[0]\n",
    "    \n",
    "    results = []\n",
    "    for i in range(logits.shape[0]):\n",
    "        result = {\n",
    "            'prediction': 'Aggregates' if predictions[i] == 1 else 'No Aggregation',\n",
    "            'confidence': confidence[i].item(),\n",
    "            'prob_no_agg': probs[i][0].item(),\n",
    "            'prob_agg': probs[i][1].item(),\n",
    "            'logit_no_agg': logits[i][0].item(),\n",
    "            'logit_agg': logits[i][1].item()\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the analysis function\n",
    "print(\"=== Prediction Analysis Results ===\")\n",
    "analysis_results = analyze_prediction(batch_logits)\n",
    "for i, result in enumerate(analysis_results):\n",
    "    print(f\"Sample {i+1}: {result['prediction']} ({result['confidence']:.3f} confidence)\")\n",
    "\n",
    "# Store predictor for later use\n",
    "print(f\"\\n=== Model Summary ===\")\n",
    "print(f\"Predictor ready for training with {trainable_params:,} trainable parameters\")\n",
    "print(f\"Input: Fused embeddings [batch_size, 1296]\")\n",
    "print(f\"Output: Aggregation logits [batch_size, 2]\")\n",
    "print(f\"Architecture: 1296 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4f624",
   "metadata": {},
   "source": [
    "## üìù Prediction Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "- **4-Block MLP Architecture**: Implemented deep neural network with progressive dimensionality reduction\n",
    "- **Regularization**: Added dropout (30%) to each block to prevent overfitting\n",
    "- **Binary Classification**: Final layer outputs logits for aggregation vs. no-aggregation prediction\n",
    "- **Batch Processing**: Demonstrated inference on multiple samples simultaneously\n",
    "- **Analysis Tools**: Created utilities for interpreting prediction results and confidence scores\n",
    "\n",
    "### Architecture Details:\n",
    "```\n",
    "Input:    [batch_size, 1296] (fused embeddings)\n",
    "Block 1:  Linear(1296‚Üí256) ‚Üí ReLU ‚Üí Dropout(0.3)\n",
    "Block 2:  Linear(256‚Üí128)  ‚Üí ReLU ‚Üí Dropout(0.3)\n",
    "Block 3:  Linear(128‚Üí64)   ‚Üí ReLU ‚Üí Dropout(0.3)  \n",
    "Block 4:  Linear(64‚Üí2)     (logits output)\n",
    "Output:   [batch_size, 2]   (aggregation predictions)\n",
    "```\n",
    "\n",
    "### Key Benefits:\n",
    "- ‚úÖ **Deep Feature Learning**: 4 layers enable complex pattern recognition\n",
    "- ‚úÖ **Regularization**: Dropout prevents overfitting during training\n",
    "- ‚úÖ **Efficient Architecture**: Progressive reduction balances capacity vs. efficiency\n",
    "- ‚úÖ **Probabilistic Output**: Softmax enables confidence estimation\n",
    "- ‚úÖ **Training Ready**: 373,314 trainable parameters optimized for aggregation prediction\n",
    "\n",
    "**Output**: Binary aggregation predictions with confidence scores\n",
    "\n",
    "### Model Pipeline Complete! üéØ\n",
    "```\n",
    "Protein Sequence ‚Üí ESM2 ‚Üí Attention Pooling ‚Üí [1280]\n",
    "                                                  ‚Üì\n",
    "Environmental Data ‚Üí MLP + BatchNorm ‚Üí [16]      ‚Üì\n",
    "                                                  ‚Üì\n",
    "                    Fusion (Concat) ‚Üí [1296] ‚Üí MLP ‚Üí [2] ‚Üí Aggregation Prediction\n",
    "```\n",
    "\n",
    "**Ready for training on aggregation datasets!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
